# Про оценку предсказательной способности

При применении методов классификации, использующих обучающие множества с неким фиксированным набором признаков, возникает задача оценки качества этих признаков.

Другими словами, мы хотели бы установить, какие значения признаков являются свойственными для класса, а какие нет, и можно ли на основе таких закономерностей делать предсказания.
Более того, мы бы хотели выразить степень этой "свойственности" в численном эквиваленте, назовем это информационным весом.


В качестве дополнительного условия потребуем, чтобы значение веса было равно 0, если с помощью одного этого признака нельзя сделать никакого вероятностного предсказания относительно класса объекта, содержащего этот признак.

Рассмотрим простой пример:

```
a,	a,	a,	c1
a,	a,	b,	c1
a,	a,	c,	c1
a,	b,	d,	c2
a,	b,	e,	c2
a,	c,	f,	c3
a,	c,	g,	c3
```

Этот набор данных представлен в формате, условно названном, C4.5. Название, насколько мне известно, закрепилось за ним из-за реализации алгоритма классификации C4.5, использующим этот формат. Через запятую перечисляются признаки объекта, каждая строка описывает один объект, последний пункт каждой строки обозначает класс объекта - т.н. метку класса, которую мы в дальнейшем и стараемся предсказать.

Первый признак имеет одно единственное значение на всем обучающем множестве, поэтому, очевидно, что с помощью него нельзя сделать никакого предсказания.

Второй и третий признаки, наоборот, имеют хорошую предсказательную силу: несложно заметить, что на всех примерах обучающего множества значение признака 2 однозначно соответствует некоторому классу.
Отличие между 2 и 3 признаками заключается лишь в том, что одному классу в первом случае соответствует единственное значение признака, а во втором - несколько, но нас пока не будет интересовать этот момент.

Для решения задачи воспользуемся некоторыми понятиями теории информации, а именно информационной энтропией.
По определению, информационная энтропия - это количество информации, приходящейся на один элемент сообщения.
Для источника сообщений, обладающим распределением вероятностей символов <em>P</em>, энтропия вычисляется согласно следующей формуле:

<div>
\[H = -\sum_{i=1}^{m}P_i log(P_i)\]
</div>

, где m - мощность алфавита. В более подробном изложении теории информации, которое можно найти, например, в курсе лекций В.С. Прохорова, приводится доказательство того факта, что значение энтропии максимально в том случае, когда вероятности появления всех признаков одинаковы, т.е. полностью рандомный источник сообщений обладает наибольшей энтропией.
Помимо простой энтропии нам также понадобится условная энтропия. Вычисление условной энтропии эквивалентно вычислению обычной энтропии с точность до вероятностей, т.е. при вычислении условной энтропии используются условные же вероятности.
В качестве примера условной энтропии возьмем, например, энтропию среди сообщений, начинающихся на букву А. За более подробными и формальными сведениями отправляю к выше упомянутому курсу лекций.
Применительно же к нашей задаче, интересно бы было посчитать энтропию меток классов при условии, что какой-то конкретный признак <em>V</em> имеет какое-то конкретное значение <em>v</em>.

<div>
$$H(C|v) = -\sum_{i=1}^{m}P(C|v) log(P(C|v))$$
</div>

Это выражение будет принимать значение 0, если значение признака однозначно определяет класс и больше нуля в противном случае. Более того, значение тем больше, чем "случайнее" распределены метки класса. Очевидно, что значение выражения будет максимальным, если значение признака не коррелирует с меткой класса (т.е. метка класса будет равномерно случайно распределена).
Умножив вероятность появления значения <em>v</em> у признака <em>V</em> на условную энтропию и просуммировав это выражения для всех вариантов значений признака <em>V</em> получим средневзвешенную условную энтропию для всего признака.

<div>
$$\sum_{v \in V} P(v) \times H(C|v) $$
</div>

Теперь нам осталось лишь удовлетворить условию соответствия отсутствия информативности у признака нулевому значению коэффициента. Для этого просто вычтем полученное на предыдущем шаге значение из полной энтропии меток класса.

<div>
$$w = H(C)-\sum_{v \in V} P(v) \times H(C|v) $$
</div>

Полученное выражение известно в литературе как <b>Information Gain</b>(IG).
У IG есть, однако, недостаток. Представим, что каждый элемент обучающего множества пронумерован уникальным порядковым номером. Оценка IG у такого признака будет велика, однако очевидно, что такой признак является плохим и с помощью него нельзя ничего предсказать.
Для учета таких ситуаций можно делить <em>w</em> на энтропию значений признака, которая тем больше, чем больше значений у признака.
Такой вариант расчета информационного веса называется <b>Information Gain Ratio</b>.

<div>
$$IG Ratio = \frac{w}{-\sum_{v \in V} P(v log(P(v)))} $$
</div>

Статистика, вычисленная для примера обучающей выборки:

```
Feats	Vals	InfoGain	GainRatio
    1      1	0.0000000	0.0000000
    2      3	1.5566567	1.0000000
    3      7	1.5566567	0.55449231
```


Следует заметить, что мы можем утверждать, что признак хороший, если IG большой, однако утверждение о том, что признак бесполезен, если его IG низкий - в общем случае не верно.
Рассмотрим еще одну обучающую выборку:


```
1,	1,	a,	a,	c1
2,	2,	a,	a,	c1
1,	2,	a,	a,	c1
2,	1,	a,	a,	c2
```


Статистика дня нее получается следующая:

```
Feats	Vals	InfoGain	GainRatio
    1      2	0.31127812	0.31127812
    2      2	0.31127812	0.31127812
    3      1	0.0000000	0.0000000
    4      1	0.0000000	0.0000000
```

Видно однако, что 1 и 2 признак зависимы и объект имеет класс c2 тогда и только тогда, когда признак 1 имеет значение 2 и признак 2 имеет значение 1.
Чтобы убедиться в этом, просто объединим 1 и 2 признаки конкатенацией (при этом, очевидно, не добавляется никакой новой информации):

```
11,	a,	a,	c1
22,	a,	a,	c1
12,	a,	a,	c1
21,	a,	a,	c2 
```

и значения информационных весов сразу возрастут:

```
Feats	Vals	InfoGain	GainRatio
    1      4	0.81127812	0.40563906
    2      1	0.0000000	0.0000000
    3      1	0.0000000	0.0000000
```

